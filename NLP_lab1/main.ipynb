{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "990d78bf-a7d6-49a9-8cef-78d5d7773cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ayanami/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ayanami/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ayanami/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer as wnl\n",
    "from nltk.tokenize import word_tokenize as wt\n",
    "from nltk.corpus import stopwords\n",
    "snow_stemmer = SnowballStemmer((\"russian\"))\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "704e0c7e-2bc9-4b5d-8b04-3b2e7952f20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text: str) -> list[str]:\n",
    "    return list(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c5eb34f-1521-4c80-aea3-a9c65e353a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(text: str) -> dict[int]:\n",
    "    vector = {i + 1: word for i, word in enumerate(text)}\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86fb0836-7d36-4863-ba95-304ce685e596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_and_stemming(text: str) -> list[str]:\n",
    "    tokens = wt(text)\n",
    "    stop_words = set(stopwords.words('russian'))\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    lem_text = [wnl().lemmatize(token, pos='v') for token in filtered_tokens]\n",
    "    print(f\"Лематизация текста:\\n {lem_text}\\n\")\n",
    "    stem_text = [snow_stemmer.stem(token) for token in lem_text]\n",
    "    print(f\"Стемминг текста:\\n {stem_text}\\n\")\n",
    "    result = ''.join(stem_text)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "baede161-5351-4023-95f3-99c6161505b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лематизация текста:\n",
      " ['nltk', '(', 'Natural', 'Language', 'ToolKit', ')', '—', 'пакет', 'библиотек', 'программ', 'символьной', 'статистической', 'обработки', 'естественного', 'языка', ',', 'написанных', 'языке', 'программирования', 'Python', '.', 'Сопровождается', 'обширной', 'документацией', ',', 'включая', 'книгу', 'объяснением', 'основных', 'концепций', ',', 'стоящих', 'теми', 'задачами', 'обработки', 'естественного', 'языка', ',', 'которые', 'выполнять', 'помощью', 'NLTK', '.']\n",
      "\n",
      "Стемминг текста:\n",
      " ['nltk', '(', 'natur', 'languag', 'toolkit', ')', '—', 'пакет', 'библиотек', 'программ', 'символьной', 'статистической', 'обработки', 'естественного', 'языка', ',', 'написанных', 'языке', 'программирования', 'python', '.', 'сопровождается', 'обширной', 'документацией', ',', 'включая', 'книгу', 'объяснением', 'основных', 'концепций', ',', 'стоящих', 'теми', 'задачами', 'обработки', 'естественного', 'языка', ',', 'которые', 'выполнять', 'помощью', 'nltk', '.']\n",
      "\n",
      "Токенизация преобразованного текста:\n",
      " ['n', 'l', 't', 'k', '(', 'n', 'a', 't', 'u', 'r', 'l', 'a', 'n', 'g', 'u', 'a', 'g', 't', 'o', 'o', 'l', 'k', 'i', 't', ')', '—', 'п', 'а', 'к', 'е', 'т', 'б', 'и', 'б', 'л', 'и', 'о', 'т', 'е', 'к', 'п', 'р', 'о', 'г', 'р', 'а', 'м', 'м', 'с', 'и', 'м', 'в', 'о', 'л', 'ь', 'н', 'о', 'й', 'с', 'т', 'а', 'т', 'и', 'с', 'т', 'и', 'ч', 'е', 'с', 'к', 'о', 'й', 'о', 'б', 'р', 'а', 'б', 'о', 'т', 'к', 'и', 'е', 'с', 'т', 'е', 'с', 'т', 'в', 'е', 'н', 'н', 'о', 'г', 'о', 'я', 'з', 'ы', 'к', 'а', ',', 'н', 'а', 'п', 'и', 'с', 'а', 'н', 'н', 'ы', 'х', 'я', 'з', 'ы', 'к', 'е', 'п', 'р', 'о', 'г', 'р', 'а', 'м', 'м', 'и', 'р', 'о', 'в', 'а', 'н', 'и', 'я', 'p', 'y', 't', 'h', 'o', 'n', '.', 'с', 'о', 'п', 'р', 'о', 'в', 'о', 'ж', 'д', 'а', 'е', 'т', 'с', 'я', 'о', 'б', 'ш', 'и', 'р', 'н', 'о', 'й', 'д', 'о', 'к', 'у', 'м', 'е', 'н', 'т', 'а', 'ц', 'и', 'е', 'й', ',', 'в', 'к', 'л', 'ю', 'ч', 'а', 'я', 'к', 'н', 'и', 'г', 'у', 'о', 'б', 'ъ', 'я', 'с', 'н', 'е', 'н', 'и', 'е', 'м', 'о', 'с', 'н', 'о', 'в', 'н', 'ы', 'х', 'к', 'о', 'н', 'ц', 'е', 'п', 'ц', 'и', 'й', ',', 'с', 'т', 'о', 'я', 'щ', 'и', 'х', 'т', 'е', 'м', 'и', 'з', 'а', 'д', 'а', 'ч', 'а', 'м', 'и', 'о', 'б', 'р', 'а', 'б', 'о', 'т', 'к', 'и', 'е', 'с', 'т', 'е', 'с', 'т', 'в', 'е', 'н', 'н', 'о', 'г', 'о', 'я', 'з', 'ы', 'к', 'а', ',', 'к', 'о', 'т', 'о', 'р', 'ы', 'е', 'в', 'ы', 'п', 'о', 'л', 'н', 'я', 'т', 'ь', 'п', 'о', 'м', 'о', 'щ', 'ь', 'ю', 'n', 'l', 't', 'k', '.']\n",
      "\n",
      "Векторизация преобразованного текста:\n",
      " {1: 'n', 2: 'l', 3: 't', 4: 'k', 5: '(', 6: 'n', 7: 'a', 8: 't', 9: 'u', 10: 'r', 11: 'l', 12: 'a', 13: 'n', 14: 'g', 15: 'u', 16: 'a', 17: 'g', 18: 't', 19: 'o', 20: 'o', 21: 'l', 22: 'k', 23: 'i', 24: 't', 25: ')', 26: '—', 27: 'п', 28: 'а', 29: 'к', 30: 'е', 31: 'т', 32: 'б', 33: 'и', 34: 'б', 35: 'л', 36: 'и', 37: 'о', 38: 'т', 39: 'е', 40: 'к', 41: 'п', 42: 'р', 43: 'о', 44: 'г', 45: 'р', 46: 'а', 47: 'м', 48: 'м', 49: 'с', 50: 'и', 51: 'м', 52: 'в', 53: 'о', 54: 'л', 55: 'ь', 56: 'н', 57: 'о', 58: 'й', 59: 'с', 60: 'т', 61: 'а', 62: 'т', 63: 'и', 64: 'с', 65: 'т', 66: 'и', 67: 'ч', 68: 'е', 69: 'с', 70: 'к', 71: 'о', 72: 'й', 73: 'о', 74: 'б', 75: 'р', 76: 'а', 77: 'б', 78: 'о', 79: 'т', 80: 'к', 81: 'и', 82: 'е', 83: 'с', 84: 'т', 85: 'е', 86: 'с', 87: 'т', 88: 'в', 89: 'е', 90: 'н', 91: 'н', 92: 'о', 93: 'г', 94: 'о', 95: 'я', 96: 'з', 97: 'ы', 98: 'к', 99: 'а', 100: ',', 101: 'н', 102: 'а', 103: 'п', 104: 'и', 105: 'с', 106: 'а', 107: 'н', 108: 'н', 109: 'ы', 110: 'х', 111: 'я', 112: 'з', 113: 'ы', 114: 'к', 115: 'е', 116: 'п', 117: 'р', 118: 'о', 119: 'г', 120: 'р', 121: 'а', 122: 'м', 123: 'м', 124: 'и', 125: 'р', 126: 'о', 127: 'в', 128: 'а', 129: 'н', 130: 'и', 131: 'я', 132: 'p', 133: 'y', 134: 't', 135: 'h', 136: 'o', 137: 'n', 138: '.', 139: 'с', 140: 'о', 141: 'п', 142: 'р', 143: 'о', 144: 'в', 145: 'о', 146: 'ж', 147: 'д', 148: 'а', 149: 'е', 150: 'т', 151: 'с', 152: 'я', 153: 'о', 154: 'б', 155: 'ш', 156: 'и', 157: 'р', 158: 'н', 159: 'о', 160: 'й', 161: 'д', 162: 'о', 163: 'к', 164: 'у', 165: 'м', 166: 'е', 167: 'н', 168: 'т', 169: 'а', 170: 'ц', 171: 'и', 172: 'е', 173: 'й', 174: ',', 175: 'в', 176: 'к', 177: 'л', 178: 'ю', 179: 'ч', 180: 'а', 181: 'я', 182: 'к', 183: 'н', 184: 'и', 185: 'г', 186: 'у', 187: 'о', 188: 'б', 189: 'ъ', 190: 'я', 191: 'с', 192: 'н', 193: 'е', 194: 'н', 195: 'и', 196: 'е', 197: 'м', 198: 'о', 199: 'с', 200: 'н', 201: 'о', 202: 'в', 203: 'н', 204: 'ы', 205: 'х', 206: 'к', 207: 'о', 208: 'н', 209: 'ц', 210: 'е', 211: 'п', 212: 'ц', 213: 'и', 214: 'й', 215: ',', 216: 'с', 217: 'т', 218: 'о', 219: 'я', 220: 'щ', 221: 'и', 222: 'х', 223: 'т', 224: 'е', 225: 'м', 226: 'и', 227: 'з', 228: 'а', 229: 'д', 230: 'а', 231: 'ч', 232: 'а', 233: 'м', 234: 'и', 235: 'о', 236: 'б', 237: 'р', 238: 'а', 239: 'б', 240: 'о', 241: 'т', 242: 'к', 243: 'и', 244: 'е', 245: 'с', 246: 'т', 247: 'е', 248: 'с', 249: 'т', 250: 'в', 251: 'е', 252: 'н', 253: 'н', 254: 'о', 255: 'г', 256: 'о', 257: 'я', 258: 'з', 259: 'ы', 260: 'к', 261: 'а', 262: ',', 263: 'к', 264: 'о', 265: 'т', 266: 'о', 267: 'р', 268: 'ы', 269: 'е', 270: 'в', 271: 'ы', 272: 'п', 273: 'о', 274: 'л', 275: 'н', 276: 'я', 277: 'т', 278: 'ь', 279: 'п', 280: 'о', 281: 'м', 282: 'о', 283: 'щ', 284: 'ь', 285: 'ю', 286: 'n', 287: 'l', 288: 't', 289: 'k', 290: '.'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"res/text.txt\", 'r') as file:\n",
    "    text = file.read().replace('\\n', '')\n",
    "result = lemmatize_and_stemming(text)\n",
    "tokenized_text = tokenize(result)\n",
    "print(f\"Токенизация преобразованного текста:\\n {tokenized_text}\\n\")\n",
    "\n",
    "vectorize_text = vectorize(result)\n",
    "print(f\"Векторизация преобразованного текста:\\n {vectorize_text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25591405-4eb4-49c2-8a10-a4e4de2f9b4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d095494-8fdc-4e30-a922-c19c6dc14a34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
